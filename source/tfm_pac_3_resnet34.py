# -*- coding: utf-8 -*-
"""tfm-pac-3_resnet34.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1caKKBoWRROaA5-FuEuLpyylPcG8YvylO

#**TFM PAC 3: IMPLEMENTACIÓ.**
#### Autor: Pau Casanova Pedrol

#**1. Càrrega de llibreries i del conjunt de dades**

## **1.1 Càrrega de llibreries**
"""

!pip install segmentation-models

pip install patchify

import os
from PIL import Image
from zipfile import ZipFile
import os
from IPython.display import display
import random
import tifffile as tiff
from patchify import patchify
import cv2
import psutil
import pickle

import numpy as np
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split

import albumentations as A

# import segmentation_models as sm
from tensorflow.keras import layers
import tensorflow as tf
import keras

print("TF version   : ", tf.__version__)
print("Keras version   : ", keras.__version__)

import sys
print(sys.version)

"""## **1.2 Càrrega d'imatges i inspecció preliminar**"""

from google.colab import drive
drive.mount('/content/drive')



with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/original_images_list.pickle', 'rb') as f:
    image_list = pickle.load(f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/original_skin_masks_list.pickle', 'rb') as f:
    skin_masks_list = pickle.load(f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/original_psor_masks_list.pickle', 'rb') as f:
    psor_masks_list = pickle.load(f)


img_number = len(image_list)
print(len(image_list))
print(len(skin_masks_list))
print(len(psor_masks_list))

images_path = '/content/drive/MyDrive/Estudis/UOC_Master-ciencia-dades/Assignatures/M2.980 - TFM/dataset/images'
skin_masks_path = '/content/drive/MyDrive/Estudis/UOC_Master-ciencia-dades/Assignatures/M2.980 - TFM/dataset/skin_masks'
psor_masks_path = '/content/drive/MyDrive/Estudis/UOC_Master-ciencia-dades/Assignatures/M2.980 - TFM/dataset/psoriasis_masks'

# Fem una inspecció preliminar a les dades
img_name_list = sorted(os.listdir(images_path))
skin_name_list = sorted(os.listdir(skin_masks_path))
psor_name_list = sorted(os.listdir(psor_masks_path))

print(len(img_name_list), len(skin_name_list), len(psor_name_list))

img_name_list[0]
patients = sorted(set([x.split('_')[0] for x in img_name_list]))

print(len(patients))
print(patients)

import pandas as pd

patients_df = pd.DataFrame(patients, columns=['Pacientes'])

# Exporting the DataFrame to an Excel file
patients_df.to_excel('/content/drive/MyDrive/Colab Notebooks/lista_pacientes.xlsx', index=False)

# Visualitzem imatges i màscares aleatòriament per a comprovar que s'han carregat correctament.
random_n = random.randint(0, img_number-1)

fig, axes = plt.subplots(1, 3, figsize=(12, 8))

axes[0].imshow(image_list[random_n])
axes[0].axis('off')
axes[0].set_title(img_name_list[random_n])

axes[1].imshow(skin_masks_list[random_n])
axes[1].axis('off')
axes[1].set_title(skin_name_list[random_n])

axes[2].imshow(psor_masks_list[random_n])
axes[2].axis('off')
axes[2].set_title(psor_name_list[random_n])

plt.savefig('/content/drive/MyDrive/Colab Notebooks/imatges_originals.jpg', format='jpg')
plt.show()
print(image_list[random_n].size)
print(skin_masks_list[random_n].size)
print(psor_masks_list[random_n].size)
print(np.array(skin_masks_list[random_n]).shape)
print(np.array(psor_masks_list[random_n]).shape)



type(image_list[23])

# Observem les dimensions màximes de totes les imatges
img_dims_list = []

for i in image_list:
    width, height = i.size
    img_dims_list.append((width, height))

print(type(img_dims_list[3]))
print(img_dims_list[:10])

max_width = max(img_dims_list, key=lambda x: x[0])[0]
max_height = max(img_dims_list, key=lambda x: x[1])[1]

print("Maximum Width:", max_width)
print("Maximum Height:", max_height)

min_width = min(img_dims_list, key=lambda x: x[0])[0]
min_height = min(img_dims_list, key=lambda x: x[1])[1]

print("Minimum Width:", min_width)
print("Minimum Height:", min_height)

image_dict = dict(zip(img_name_list, img_dims_list))

print(image_dict)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/image_dict.pickle', 'wb') as f: pickle.dump(image_dict, f)

"""# **2. Preprocessament d'imatges**

## **2.1 Creació de màscares multicategòriques**
"""

# Comprovem quins valors prenen els píxels de les imatges
random_n = random.randint(0, img_number-1)
print(np.unique(skin_masks_list[random_n]))
print(np.unique(psor_masks_list[random_n]))

# Creem una funció que a partir de dues llistes de màscares, les sobreposi creant una màscara multicategòrica on el valor 0 representi un píxel de fons, el valor 1 representi un píxel de pell sana,
# i el valor 2 representi un píxel de pell amb psoriasi

def multiclass_masks(mask_list1, mask_list2):
  multiclass_masks_list = []
  for i in range(len(mask_list1)):
    mask1 = np.array(mask_list1[i])
    mask2 = np.array(mask_list2[i])
    multiclass_mask = np.zeros_like(mask1)

    multiclass_mask[(mask1 == 255) & (mask2 == 0)] = 1 # Categoria pell
    multiclass_mask[(mask1 == 255) & (mask2 == 255)] = 2 # Categoria psoriasi

    multiclass_masks_list.append(Image.fromarray(multiclass_mask))

  return multiclass_masks_list

multiclass_masks_list = multiclass_masks(skin_masks_list, psor_masks_list)

# Observem aleatòriament els valors únics d'algunes màscares generades per comprovar que han pres els valors correctes
random_n = random.randint(0, img_number-1)

print(len(multiclass_masks_list))
print(np.unique(multiclass_masks_list[random_n]))
print(type(multiclass_masks_list[random_n]))
print(np.array(multiclass_masks_list[random_n]).shape)

# Visualitzem aleatòriament les màscares multicategòriques generades
random_n = random.randint(0, img_number-1)

fig, axes = plt.subplots(1, 4, figsize=(12, 4))

axes[0].imshow(image_list[random_n])
axes[0].axis('off')

axes[1].imshow(skin_masks_list[random_n])
axes[1].axis('off')

axes[2].imshow(psor_masks_list[random_n])
axes[2].axis('off')

axes[3].imshow(multiclass_masks_list[random_n], cmap = 'jet')
axes[3].axis('off')

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/original_multiclass_masks_list.pickle', 'wb') as f: pickle.dump(multiclass_masks_list, f)

"""## **2.2 Preprocessament imatges: redimensionament**
#### Reescalem les imatges i les màscares.
"""

# Definim una funció que escali les imatges originals per a que tinguin la mida que volem

def resize_images(image_list, width, height):
    array_list = []
    for i in range(len(image_list)):
        resized_image = image_list[i].resize((width,height), Image.LANCZOS)
        array_list.append(np.array(resized_image))

    return np.array(array_list)

# Definim una funció que escali les màscares originals per a que tinguin la mida que volem
def resize_masks(mask_list, width, height):
    array_list = []
    for i in range(len(mask_list)):
        resized_mask = mask_list[i].resize((width, height), Image.LANCZOS)
        array_list.append(np.array(resized_mask))

    return np.array(array_list)

# Executem la funció per les imatges
resized_images = resize_images(image_list, 512, 512)

# Executem la funció per les màscares
resized_masks = resize_masks(multiclass_masks_list, 512, 512)

print(type(resized_images))
print(resized_images.shape)

print(type(resized_masks))
print(resized_masks.shape)

# Mostrem les imatges resultants del preprocessament
fig, axes = plt.subplots(1, 2, figsize=(10, 10))

random_n = random.randint(0, img_number-1)

axes[0].imshow(resized_images[random_n])
axes[0].axis('off')

axes[1].imshow(resized_masks[random_n])
axes[1].axis('off')

"""----------------------------------------------------------------------------------------------------------------------------------------------

## **2.3 Preprocessament imatges: normalització de dades i nombre de canals.**
#### Modifiquem totes les imatges per a que prenguin valors entre 0 i 1 i les màscares per a que tinguin 1 canal.

#### Modificació de les imatges
"""

# Mostrem els valors màxims i mínims que prenen les imatges
max_img = np.amax(resized_images)
min_img = np.amin(resized_images)

print(max_img, min_img)

# Comprovem l'ús de memoria RAM
memory_info = psutil.virtual_memory()
print(f"Current memory usage: {memory_info.percent}%")

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/resized_images_E.pickle', 'wb') as f: pickle.dump(resized_images, f)
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/resized_masks_E.pickle', 'wb') as f: pickle.dump(resized_masks, f)

import psutil
import pickle
import numpy as np
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
import random

resized_images_path = "/content/drive/MyDrive/Colab Notebooks/TFM_variables/resized_images_E.pickle"

with open(resized_images_path, 'rb') as f:
    resized_images = pickle.load(f)

# Check current memory usage
memory_info = psutil.virtual_memory()
print(f"Current memory usage: {memory_info.percent}%")

# Normalitzem les matrius de les imatges per a que prenguin valors entre 0 i 1.
norm_images = np.array([x/255.0 for x in resized_images])

# Check current memory usage
memory_info = psutil.virtual_memory()
print(f"Current memory usage: {memory_info.percent}%")

max_img = np.amax(norm_images)
min_img = np.amin(norm_images)

print(max_img, min_img)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/norm_images_E.pickle', 'wb') as f: pickle.dump(norm_images, f)

print(type(norm_images))
print(norm_images.shape)
print(type(norm_images[0]))
print(norm_images[0].shape)

print(np.max(norm_images[random.randint(0, 338)]))
print(np.min(norm_images[random.randint(0, 338)]))

"""#### Modificació de les màscares"""

resized_masks_path = "/content/drive/MyDrive/Colab Notebooks/TFM_variables/resized_masks_E.pickle"
with open(resized_masks_path, 'rb') as f:
    resized_masks = pickle.load(f)

resized_masks.shape

# Modifiquem el nombre de canals per a que les màscares tinguin dimensions (339, 512, 512, 1)
reshaped_masks = resized_masks.reshape((339, 512, 512, 1))

print(reshaped_masks.shape)

# Comprovem visualment el resultat de la transformació
random_n = random.randint(0, reshaped_masks.shape[0]-1)
plt.imshow(reshaped_masks[random_n, :, :,:])
plt.axis('off')

# Comprovem que les màscares prenen només els valors 0, 1 i 2.
np.unique(reshaped_masks[random.randint(0, reshaped_masks.shape[0]-1), :, :,:])

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/reshaped_masks_E.pickle', 'wb') as f: pickle.dump(reshaped_masks, f)

"""# **3. Partició de les imatges en conjunts d'entrenament i test**"""

import segmentation_models as sm
import pickle
from sklearn.model_selection import train_test_split
import numpy as np

# Càrrega de les variables que es faran servir
norm_images_path = "/content/drive/MyDrive/Colab Notebooks/TFM_variables/norm_images_E.pickle"
with open(norm_images_path, 'rb') as f:
    norm_images = pickle.load(f)

reshaped_masks_path = "/content/drive/MyDrive/Colab Notebooks/TFM_variables/reshaped_masks_E.pickle"
with open(reshaped_masks_path, 'rb') as f:
    reshaped_masks = pickle.load(f)

# Preprocessem l'input
BACKBONE = 'resnet34'
preprocess_input = sm.get_preprocessing(BACKBONE)
prep_images = preprocess_input(norm_images)

from keras.utils import to_categorical

# Convertim les màscares per a que tinguin 3 canals, un per categoria
cat_masks = to_categorical(reshaped_masks, num_classes=3)

cat_masks.shape

with open("/content/drive/MyDrive/Colab Notebooks/TFM_variables/image_dict.pickle", 'rb') as f:
    image_dict = pickle.load(f)

image_data = [[key, value[0], value[1]] for key, value in image_dict.items()]

print(image_data[0])

# Separem les imatges en els subconjunts d'entrenament i test + validació
X_train, X_test_val, y_train, y_test_val = train_test_split(prep_images, cat_masks, test_size = 0.5, random_state = 42)
data_train, data_test_val = train_test_split(image_data, test_size = 0.5, random_state = 42)

print(X_train.shape)
print(X_test_val.shape)
print(y_train.shape)
print(y_test_val.shape)

len(data_train)

# Mostrem les imatges per veure si es corresponen amb els noms.
import random
from matplotlib import pyplot as plt

random_n = random.randint(0, len(X_train)-1)

plt.imshow(X_train[random_n])
plt.axis('off')
plt.title(str(data_train[random_n][0]) + " dimensions: " + str(data_train[random_n][1]) + "x" + str(data_train[random_n][2]))

# Mostrem les imatges per veure si es corresponen amb els noms.
random_n = random.randint(0, len(X_test_val)-1)

plt.imshow(X_test_val[random_n])
plt.axis('off')
plt.title(str(data_test_val[random_n][0]) + " dimensions: " + str(data_test_val[random_n][1]) + "x" + str(data_test_val[random_n][2]))

# Separem a la seva vegada el subconjunt de test + validació en subconjunts de test i validació
X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size = 0.5, random_state = 42)
data_test, data_val = train_test_split(data_test_val, test_size = 0.5, random_state = 42)

print(X_test.shape)
print(X_val.shape)
print(y_test.shape)
print(y_val.shape)

# Mostrem les imatges per veure si es corresponen amb els noms.
random_n = random.randint(0, len(X_test)-1)

plt.imshow(X_test[random_n])
plt.axis('off')
plt.title(str(data_test[random_n][0]) + " dimensions: " + str(data_test[random_n][1]) + "x" + str(data_test[random_n][2]))

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/X_train_E.pickle', 'wb') as f: pickle.dump(X_train, f)
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/X_test_E.pickle', 'wb') as f: pickle.dump(X_test, f)
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/X_val_E.pickle', 'wb') as f: pickle.dump(X_val, f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/y_train_E.pickle', 'wb') as f: pickle.dump(y_train, f)
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/y_test_E.pickle', 'wb') as f: pickle.dump(y_test.astype(np.float32), f)
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/y_val_E.pickle', 'wb') as f: pickle.dump(y_val.astype(np.float32), f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/data_train_E.pickle', 'wb') as f: pickle.dump(data_train, f)
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/data_test_E.pickle', 'wb') as f: pickle.dump(data_test, f)
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/data_val_E.pickle', 'wb') as f: pickle.dump(data_val, f)

"""# **4. Data augmentation**
Com que el dataset original del que disposem és relativament petit (339 imatges originals, amb 339 màscares de la pell i 339 màscares de la psoriasi), i és complicat aconseguir més imatges etiquetades, farem servir tècniques de data augmentation per millorar el rendiment del model.
"""

import pickle

# Càrrega de les variables que es faran servir
X_train_path = "/content/drive/MyDrive/Colab Notebooks/TFM_variables/X_train_E.pickle"
with open(X_train_path, 'rb') as f:
    X_train = pickle.load(f)

y_train_path = "/content/drive/MyDrive/Colab Notebooks/TFM_variables/y_train_E.pickle"
with open(y_train_path, 'rb') as f:
    y_train = pickle.load(f)

import albumentations as A

aug = A.Compose([
    A.VerticalFlip(p=0.5),
    A.RandomRotate90(p=0.5),
    A.HorizontalFlip(p=0.5),
    A.Transpose(p=1),
    A.GridDistortion(p=1)
    ]
)

import random
import numpy as np

seed = 13

def augment_data (images, masks, images_to_generate):
    augmented_images = []
    augmented_masks = []

    for i in range(images_to_generate):
        random_n = random.randint(0, len(images)-1)
        image = images[random_n]
        mask = masks[random_n]
        augmented_image_mask = aug(image=image, mask=mask)
        transformed_image = augmented_image_mask['image']
        transformed_mask = augmented_image_mask['mask']

        augmented_images.append(transformed_image)
        augmented_masks.append(transformed_mask)


    return np.array(augmented_images), np.array(augmented_masks)

generated_xtrain, generated_ytrain = augment_data(X_train, y_train, 510)

random_n = random.randint(0, len(generated_xtrain)-1)

print(type(generated_xtrain[random_n]))
print(generated_xtrain[random_n].shape)
print(type(generated_ytrain[random_n]))
print(generated_ytrain[random_n].shape)

from matplotlib import pyplot as plt
from IPython.display import display

# Mostrem les imatges creades artificialment.
random_n = random.randint(0, len(generated_xtrain)-1)

fig, axes = plt.subplots(1, 2, figsize=(12, 12))

axes[0].imshow(generated_xtrain[random_n])
axes[0].axis('off')

axes[1].imshow(generated_ytrain[random_n], cmap='jet')
axes[1].axis('off')

print(type(X_train))
print(X_train.shape)
print(type(y_train))
print(y_train.shape)

print(type(generated_xtrain))
print(generated_xtrain.shape)
print(type(generated_ytrain))
print(generated_ytrain.shape)

# Unim les imatges generades amb les imatges generals, per obtenir un dataset final amb
aug_X_train = np.concatenate((X_train, generated_xtrain), axis=0).astype(np.float32)
aug_y_train = np.concatenate((y_train, generated_ytrain), axis=0).astype(np.float32)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/aug_X_train_E.pickle', 'wb') as f: pickle.dump(aug_X_train, f)
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/aug_y_train_E.pickle', 'wb') as f: pickle.dump(aug_y_train, f)

"""# **5. Implementació del model**

#### Preprocessament final
"""

import pickle
import numpy as np

# Càrrega de les variables que es faran servir
aug_X_train_path = "/content/drive/MyDrive/Colab Notebooks/TFM_variables/aug_X_train_E.pickle"
with open(aug_X_train_path, 'rb') as f:
    aug_X_train = pickle.load(f)

aug_y_train_path = "/content/drive/MyDrive/Colab Notebooks/TFM_variables/aug_y_train_E.pickle"
with open(aug_y_train_path, 'rb') as f:
    aug_y_train = pickle.load(f)

X_test_path = "/content/drive/MyDrive/Colab Notebooks/TFM_variables/X_test_E.pickle"
with open(X_test_path, 'rb') as f:
    X_test = pickle.load(f)

y_test_path = "/content/drive/MyDrive/Colab Notebooks/TFM_variables/y_test_E.pickle"
with open(y_test_path, 'rb') as f:
    y_test = pickle.load(f).astype(np.float32)

X_val_path = "/content/drive/MyDrive/Colab Notebooks/TFM_variables/X_val_E.pickle"
with open(X_val_path, 'rb') as f:
    X_val = pickle.load(f)

y_val_path = "/content/drive/MyDrive/Colab Notebooks/TFM_variables/y_val_E.pickle"
with open(y_val_path, 'rb') as f:
    y_val = pickle.load(f).astype(np.float32)

print(aug_X_train.shape, aug_y_train.shape, X_test.shape, y_test.shape, X_val.shape, y_val.shape)
print(aug_X_train.dtype, aug_y_train.dtype, X_test.dtype, y_test.dtype, X_val.dtype, y_val.dtype)

"""----------------------------------------------------------------------------------------------------------------------

#### Compilació del model
"""

from tensorflow.keras import layers
import tensorflow as tf
import keras
import segmentation_models as sm
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback

BACKBONE = 'resnet34'
activation = 'softmax'
LR = 0.0001
optim = keras.optimizers.Adam(LR)
dice_loss = sm.losses.DiceLoss()
metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]

model = sm.Unet(BACKBONE, encoder_weights='imagenet', classes=3, activation=activation, encoder_freeze = False)
model.compile(optim, dice_loss, metrics=metrics)

print(model.summary())

"""#### Callbacks"""

from timeit import default_timer as timer

class TimerCallback(Callback):
    def __init__(self):
        self.counter = 0

    def on_train_begin(self, logs=None):
        self.start_time = timer()

    def on_epoch_end(self, epoch, logs=None):
        self.counter += 1

    def on_train_end(self, logs=None):
        self.end_time = timer()
        self.temps_total_seg = self.end_time - self.start_time

        temps_h = int(self.temps_total_seg // 3600)
        temps_mins = int((self.temps_total_seg % 3600)// 60)
        temps_seg = int((self.temps_total_seg % 3600) % 60)
        self.temps_epoca =  (self.temps_total_seg)/(self.counter)
        num_epochs = "El model ha estat entrenat {} èpoques".format(self.counter)
        training_time = "El temps total d'entrenament ha estat de {} hores, {} minuts i {} segons".format(temps_h, temps_mins, temps_seg)
        time_per_epoch = f"El temps d'entrenament per època ha estat de {self.temps_epoca:.2f} segons"
        print(num_epochs)
        print(training_time)
        print(time_per_epoch)

from IPython.display import clear_output

class PlotLearning(Callback):
    """
    Callback para plotear las métricas durante el entrenamiento.
    """
    def __init__(self, showLR=False):
      self.showLR = showLR                   # podem escollir si mostrar o no el plot del learning rate en cada epoch

    def on_train_begin(self, logs={}):
        self.metrics = {}
        for metric in logs:
            self.metrics[metric] = []


    def on_epoch_end(self, epoch, logs={}):
        for metric in logs:
            if metric in self.metrics:
                self.metrics[metric].append(logs.get(metric))
            else:
                self.metrics[metric] = [logs.get(metric)]

        metric = [x for x in logs if ('val' not in x) and ('lr' not in x)]
        if self.showLR:
          metric.append('lr')

        f, axs = plt.subplots(1,len(metric),figsize=(10,4))
        clear_output(wait=True)
        for i,ax in enumerate(axs):
          ax.plot(range(1, epoch + 2), self.metrics[metric[i]],'o--',label=metric[i])
          try:
            ax.plot(range(1, epoch + 2), self.metrics['val_' + metric[i]], 'o--', label='val_' + metric[i])
          except:
            pass
          ax.set_xlabel('# epochs')
          ax.set_ylabel(metric[i])
          ax.legend()
          ax.grid()

        plt.tight_layout()
        plt.show()

"""#### Entrenament del model"""

from matplotlib import pyplot as plt

early_stopping = EarlyStopping(monitor='val_loss', patience = 10)
plot_learning = PlotLearning()
timer_callback = TimerCallback()

history = model.fit(aug_X_train,
                    aug_y_train,
                    batch_size=8,
                    epochs=100,
                    verbose=1,
                    callbacks=[early_stopping, plot_learning, timer_callback],
                    validation_data=(X_val, y_val))

model.save('/content/drive/MyDrive/Colab Notebooks/TFM_variables/unet_resnet34_E.keras')

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/history_E.pickle', 'wb') as f: pickle.dump(history, f)
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/plot_learning_E.pickle', 'wb') as f: pickle.dump(plot_learning, f)
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/timer_callback_E.pickle', 'wb') as f: pickle.dump(timer_callback, f)

import segmentation_models as sm
from keras.models import load_model
import os

model = load_model('/content/drive/MyDrive/Colab Notebooks/TFM_variables/unet_resnet34_E.keras', compile=False)

print(history.history.keys())

from matplotlib import pyplot as plt

def plotmetrics(hist):
    # Gràfica accuracy entrenament i validació.
    plt.plot(hist.history['iou_score'])
    plt.plot(hist.history['val_iou_score'])
    plt.title('IOU train vs. validation')
    plt.ylabel('IOU')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

    # Gràfica loss entrenament i validació.
    plt.plot(hist.history['loss'])
    plt.plot(hist.history['val_loss'])
    plt.title('Loss train vs. validation')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

plotmetrics(history)

"""# **6. Avaluació dels resultats**"""

y_pred=model.predict(X_test)

y_pred_val=model.predict(X_val)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/y_pred_E.pickle', 'wb') as f: pickle.dump(y_pred, f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/y_pred_val_E.pickle', 'wb') as f: pickle.dump(y_pred_val, f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/y_pred_E.pickle', 'rb') as f:
    y_pred = pickle.load(f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/y_test_E.pickle', 'rb') as f:
    y_test = pickle.load(f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/X_test_E.pickle', 'rb') as f:
    X_test = pickle.load(f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/data_test_E.pickle', 'rb') as f:
    data_test = pickle.load(f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/y_pred_val_E.pickle', 'rb') as f:
    y_pred_val = pickle.load(f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/y_val_E.pickle', 'rb') as f:
    y_val = pickle.load(f)

import numpy as np
y_pred_argmax = np.argmax(y_pred, axis=3)
y_test_argmax = np.argmax(y_test, axis=3)

import numpy as np
y_pred_val_argmax = np.argmax(y_pred_val, axis=3)
y_val_argmax = np.argmax(y_val, axis=3)

print(np.unique(y_test))
print(y_test.shape)

print(np.unique(y_test_argmax))
print(y_test_argmax.shape)

print(type(y_pred))
print(y_pred.shape)

print(type(y_pred_argmax))
print(np.unique(y_pred_argmax))
print(y_pred_argmax.shape)

random_n = random.randint(0, len(y_pred_argmax)-1)

fig, axes = plt.subplots(1, 2, figsize=(12, 12))

axes[0].imshow(y_test_argmax[random_n])
axes[0].axis('off')
axes[0].set_title('y_test_argmax')

axes[1].imshow(y_pred_argmax[random_n])
axes[1].axis('off')
axes[1].set_title('y_pred_argmax')

# Calculem la IoU mitjana
from keras.metrics import MeanIoU

n_classes = 3
meanIOU = MeanIoU(num_classes=n_classes)
meanIOU.update_state(y_test_argmax, y_pred_argmax)
print(meanIOU.result().numpy())

# Calculem la IoU de cada classe
values = np.array(meanIOU.get_weights()).reshape(n_classes, n_classes)

class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[1,0]+ values[2,0])
class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[0,1]+ values[2,1])
class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[0,2]+ values[1,2])

print("IoU for class1 is: ", class1_IoU)
print("IoU for class2 is: ", class2_IoU)
print("IoU for class3 is: ", class3_IoU)

test_result = model.evaluate(X_test, y_test, batch_size=8)

print(test_result)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/test_result_E.pickle', 'wb') as f: pickle.dump(test_result, f)

print(y_pred.shape)
print(y_test.shape)

import random
from matplotlib import pyplot as plt

# Comparem la imatge original, amb el ground truth i la màscara generada pel nostre model
random_n = random.randint(0, len(X_test)-1)

fig, axes = plt.subplots(1, 3, figsize=(12, 12))

axes[0].imshow(X_test[random_n])
axes[0].axis('off')

axes[1].imshow(y_test[random_n], cmap='jet')
axes[1].axis('off')

axes[2].imshow(y_pred[random_n], cmap='jet')
axes[2].axis('off')

"""# **7. Postprocessat imatges**"""

from PIL import Image

def enlarge_masks (masks, data):
  array_list = []
  for i in range(len(masks)):
    new_width = data[i][1]
    new_height = data[i][2]
    mask_array = masks[i]/2
    mask_image = Image.fromarray((mask_array))
    enlarged_mask = mask_image.resize((new_width, new_height), Image.LANCZOS)
    enlarged_mask_array = np.array(enlarged_mask)

    enlarged_mask_array[enlarged_mask_array < 0.2] = 0.0
    enlarged_mask_array[enlarged_mask_array > 0.8] = 2.0
    enlarged_mask_array[(enlarged_mask_array >= 0.2) & (enlarged_mask_array <= 0.8)] = 1.0

    array_list.append(enlarged_mask_array)

  return array_list

from PIL import Image

enlarged_y_pred = enlarge_masks(y_pred_argmax, data_test)
enlarged_y_test = enlarge_masks(y_test_argmax, data_test)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/data_val_E.pickle', 'rb') as f:
    data_val = pickle.load(f)

enlarged_y_pred_val = enlarge_masks(y_pred_val_argmax, data_val)
enlarged_y_val = enlarge_masks(y_val_argmax, data_val)

random_n =random.randint(0, len(enlarged_y_test)-1)

rand_y_test_enl = enlarged_y_test[random_n]
rand_y_pred_enl = enlarged_y_pred[random_n]

print(rand_y_test_enl.shape)
print(rand_y_test_enl.dtype)
print(np.unique(rand_y_test_enl))

print(rand_y_pred_enl.shape)
print(rand_y_pred_enl.dtype)
print(np.unique(rand_y_pred_enl))

fig, axes = plt.subplots(1, 2, figsize=(12, 12))

axes[0].imshow(rand_y_test_enl)
axes[0].axis('off')

axes[1].imshow(rand_y_pred_enl)
axes[1].axis('off')

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/enlarged_y_test_E.pickle', 'wb') as f: pickle.dump(enlarged_y_test, f)
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/enlarged_y_pred_E.pickle', 'wb') as f: pickle.dump(enlarged_y_pred, f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/enlarged_y_pred_val_E.pickle', 'wb') as f: pickle.dump(enlarged_y_pred_val, f)
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/enlarged_y_val_E.pickle', 'wb') as f: pickle.dump(enlarged_y_val, f)

import pickle

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/data_test_E.pickle', 'rb') as f:
    data_test = pickle.load(f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/enlarged_y_test_E.pickle', 'rb') as f:
    enlarged_y_test = pickle.load(f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/enlarged_y_pred_E.pickle', 'rb') as f:
    enlarged_y_pred = pickle.load(f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/enlarged_y_pred_val_E.pickle', 'rb') as f:
    enlarged_y_pred_val = pickle.load(f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/enlarged_y_val_E.pickle', 'rb') as f:
    enlarged_y_pred_val = pickle.load(f)

def calculate_psor_area (masks_list):
  psoriasis_areas = []
  for i in range(len(masks_list)):
    values, counts = np.unique(masks_list[i], return_counts=True)
    if len(counts) < 3:
      psoriasis_areas.append(0.0)
    else:
      psor_area = counts[2] / (counts[1] + counts[2])
      psoriasis_areas.append(psor_area)

  return psoriasis_areas

print(type(enlarged_y_pred))
print(len(enlarged_y_pred))

print(type(enlarged_y_pred_val))
print(len(enlarged_y_pred_val))

enlarged_y_pred_full = enlarged_y_pred + enlarged_y_pred_val
print(type(enlarged_y_pred_full))
print(len(enlarged_y_pred_full))

enlarged_y_full = enlarged_y_test + enlarged_y_val

import numpy as np

test_psor_areas = calculate_psor_area(enlarged_y_full)
pred_psor_areas = calculate_psor_area(enlarged_y_pred_full)

data_full = data_test + data_val

image_names = [x[0] for x in data_full]

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/test_psor_areas_E.pickle', 'wb') as f: pickle.dump(test_psor_areas, f)
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/pred_psor_areas_E.pickle', 'wb') as f: pickle.dump(pred_psor_areas, f)
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/data_full_E.pickle', 'wb') as f: pickle.dump(data_full, f)
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/image_names_E.pickle', 'wb') as f: pickle.dump(image_names, f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/test_psor_areas_E.pickle', 'rb') as f:
    test_psor_areas = pickle.load(f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/pred_psor_areas_E.pickle', 'rb') as f:
    pred_psor_areas = pickle.load(f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/data_full_E.pickle', 'rb') as f:
    data_full = pickle.load(f)

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/image_names_E.pickle', 'rb') as f:
    image_names = pickle.load(f)

patients = [x.split('_')[0] for x in image_names]
body_parts = [x.split('_')[1].split('.')[0] for x in image_names]

patient_bodypart = [x.split('.')[0] for x in image_names]

patient_bodypart[:10]

print(len(patient_bodypart))

import random
from matplotlib import pyplot as plt

random_n =random.randint(0, len(enlarged_y_test)-1)

plt.imshow(enlarged_y_test[random_n])
plt.title(patients[random_n] + "_" + body_parts[random_n])

import pandas as pd

psoriasis_areas_df = pd.DataFrame(list(zip(patient_bodypart, patients, body_parts, test_psor_areas, pred_psor_areas)),
                  columns=['image', 'patient_code', 'body_part', 'ground_truth%', 'predicted%'])

psoriasis_areas_df

with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/psoriasis_areas_df_E.pickle', 'wb') as f: pickle.dump(psoriasis_areas_df, f)

import pickle
with open('/content/drive/MyDrive/Colab Notebooks/TFM_variables/psoriasis_areas_df_E.pickle', 'rb') as f:
    psoriasis_areas_df = pickle.load(f)

df_sorted =psoriasis_areas_df.sort_values(by='image')

df_sorted

unique_patients = set(patients)
print(unique_patients)
print(len(unique_patients))

patient_dataframes = []

for i in unique_patients:
  patient_df = psoriasis_areas_df[psoriasis_areas_df['patient_code'] == i]
  patient_dataframes.append(patient_df)

import random
random_n =random.randint(0, len(patient_dataframes)-1)

print(patient_dataframes[random_n])
print(random_n)

len(patient_dataframes)

def filter_dataframes(original_dataframe):
  filtered_dataframes = []

  for i in range(len(original_dataframe)):
    df = original_dataframe[i]
    image = df.iloc[0, 0]
    patient_code = df.iloc[0, 1]
    body_parts = df['body_part'].tolist()
    EID_strings = ['EIDA', 'EIDP']
    EII_strings = ['EIIA', 'EIIP']
    ESD_strings = ['ESDA', 'ESDP']
    ESI_strings = ['ESIA', 'ESIP']

    if all(s in body_parts for s in EID_strings):
      ground_truth_EIDA = df.loc[df.index[df['body_part'] == 'EIDA'][0], 'ground_truth%']
      ground_truth_EIDP = df.loc[df.index[df['body_part'] == 'EIDP'][0], 'ground_truth%']
      predicted_EIDA = df.loc[df.index[df['body_part'] == 'EIDA'][0], 'predicted%']
      predicted_EIDP = df.loc[df.index[df['body_part'] == 'EIDP'][0], 'predicted%']
      ground_truth_EID = (ground_truth_EIDA+ground_truth_EIDP)/2
      predicted_EID = (predicted_EIDA+predicted_EIDP)/2
      df = df.append({'image': (patient_code + "_" + 'EID'), 'patient_code': patient_code, 'body_part': 'EID', 'ground_truth%' : ground_truth_EID, 'predicted%': predicted_EID}, ignore_index=True)
      df = df[df['body_part'] != 'EIDA']
      df = df[df['body_part'] != 'EIDP']

    if all(s in body_parts for s in EII_strings):
      ground_truth_EIIA = df.loc[df.index[df['body_part'] == 'EIIA'][0], 'ground_truth%']
      ground_truth_EIIP = df.loc[df.index[df['body_part'] == 'EIIP'][0], 'ground_truth%']
      predicted_EIIA = df.loc[df.index[df['body_part'] == 'EIIA'][0], 'predicted%']
      predicted_EIIP = df.loc[df.index[df['body_part'] == 'EIIP'][0], 'predicted%']
      ground_truth_EII = (ground_truth_EIIA+ground_truth_EIIP)/2
      predicted_EII = (predicted_EIIA+predicted_EIIP)/2
      df = df.append({'image': (patient_code + "_" + 'EII'), 'patient_code': patient_code, 'body_part': 'EII', 'ground_truth%' : ground_truth_EII, 'predicted%': predicted_EII}, ignore_index=True)
      df = df[df['body_part'] != 'EIIA']
      df = df[df['body_part'] != 'EIIP']

    if all(s in body_parts for s in ESD_strings):
      ground_truth_ESDA = df.loc[df.index[df['body_part'] == 'ESDA'][0], 'ground_truth%']
      ground_truth_ESDP = df.loc[df.index[df['body_part'] == 'ESDP'][0], 'ground_truth%']
      predicted_ESDA = df.loc[df.index[df['body_part'] == 'ESDA'][0], 'predicted%']
      predicted_ESDP = df.loc[df.index[df['body_part'] == 'ESDP'][0], 'predicted%']
      ground_truth_ESD = (ground_truth_ESDA+ground_truth_ESDP)/2
      predicted_ESD = (predicted_ESDA+predicted_ESDP)/2
      df = df.append({'image': (patient_code + "_" + 'ESD'), 'patient_code': patient_code, 'body_part': 'ESD', 'ground_truth%' : ground_truth_ESD, 'predicted%': predicted_ESD}, ignore_index=True)
      df = df[df['body_part'] != 'ESDA']
      df = df[df['body_part'] != 'ESDP']

    if all(s in body_parts for s in ESI_strings):
      ground_truth_ESIA = df.loc[df.index[df['body_part'] == 'ESIA'][0], 'ground_truth%']
      ground_truth_ESIP = df.loc[df.index[df['body_part'] == 'ESIP'][0], 'ground_truth%']
      predicted_ESIA = df.loc[df.index[df['body_part'] == 'ESIA'][0], 'predicted%']
      predicted_ESIP = df.loc[df.index[df['body_part'] == 'ESIP'][0], 'predicted%']
      ground_truth_ESI = (ground_truth_ESIA+ground_truth_ESIP)/2
      predicted_ESI = (predicted_ESIA+predicted_ESIP)/2
      df = df.append({'image': (patient_code + "_" + 'ESI'), 'patient_code': patient_code, 'body_part': 'ESI', 'ground_truth%' : ground_truth_ESI, 'predicted%': predicted_ESI}, ignore_index=True)
      df = df[df['body_part'] != 'ESIA']
      df = df[df['body_part'] != 'ESIP']

    filtered_dataframes.append(df)

  return filtered_dataframes

filtered_dataframes = filter_dataframes(patient_dataframes)

len(filtered_dataframes)

import random

random_n =random.randint(0, len(filtered_dataframes)-1)

filtered_dataframes[random_n]

import pandas as pd

filtered_df = pd.concat(filtered_dataframes, ignore_index=True)

filtered_df.sort_values(by='image')

filtered_df.shape[0]

"""# **8. Comparació resultats metges**"""

med_data = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/Recollida de dades def tots (1).xlsx')

med_data = med_data.iloc[:, [0, 9, 14, 19, 24, 29, 34]].drop(index=[0, 1, 2, 3])

med_data.columns = ['patient', 'ESD', 'ESI', 'TA', 'TP', 'EID', 'EII']

med_data.sort_values(by='patient')

rows_as_lists = med_data.values.tolist()

column_names_list = med_data.columns.tolist()

expanded_list = []

for r in rows_as_lists:
  for i in range(1,6):
      patient_bodypart_value = [str(r[0]) + "_" + column_names_list[i]]
      patient_bodypart_value.append(r[i])
      expanded_list.append(patient_bodypart_value)

print(expanded_list[:20])
print(len(expanded_list))

med_df = pd.DataFrame(expanded_list, columns=['image', 'doctor%'])

med_df['doctor%'] = med_df['doctor%']/100
med_df.sort_values(by='image')

med_df2 = med_df.drop_duplicates(subset='image', keep=False).dropna()

med_df2.shape[0]

final_df = pd.merge(filtered_df, med_df2, on='image', how='inner')

final_df.sort_values(by='image')

final_df['diff_GT_predicted'] = final_df['ground_truth%'] - final_df['predicted%']
final_df['diff_GT_doctor'] = final_df['ground_truth%'] - final_df['doctor%']

final_df

final_df['max'] = final_df[['ground_truth%', 'predicted%', 'doctor%']].max(axis=1)
final_df['min'] = final_df[['ground_truth%', 'predicted%', 'doctor%']].min(axis=1)

final_df.shape[0]

final_df

from matplotlib import pyplot as plt

fig, ax = plt.subplots(figsize=(10,20), facecolor = "white")

ax.grid(which="major", axis='both', color='#758D99', alpha=0.6, zorder=1)
ax.spines[['top','right','bottom']].set_visible(False)

ax.hlines(y=final_df['image'], xmin=final_df['min'], xmax=final_df['max'], color='#758D99', zorder=2, linewidth=2, label='_nolegend_', alpha=.8)

ax.scatter(x=final_df['ground_truth%'], y=final_df['image'], label='ground truth', s=60, color='#DB444B', zorder=3)
ax.scatter(x=final_df['doctor%'], y=final_df['image'], label='psoriasis', s=60, color='#006BA2', zorder=3)
ax.scatter(x=final_df['predicted%'], y=final_df['image'], label='psoriasis', s=60, color='#fac011', zorder=3)


ax.xaxis.set_tick_params(labeltop=True, labelbottom=False, bottom=False, labelsize=11, pad=-1)

ax.set_yticks( final_df['image'])
ax.set_yticklabels( final_df['image'], ha = 'left')
ax.yaxis.set_tick_params(pad=120, labelsize=11)

ax.legend(['ground truth', 'doctor', 'predicción'], loc=(0,0), ncol=3)

ax.text(x=-0.04, y=0.9, s="Comparación % psoriasis ground truth/predicción/médico", transform=fig.transFigure, ha='left', fontsize=13, weight='bold', alpha=.8)

"""Referencia código gráfico tipo "Dumbbell":
https://gist.github.com/ruthpozuelo/16a74027904dcd73d37df5512bb0325c#file-dumbbell_chart-ipynb
"""

